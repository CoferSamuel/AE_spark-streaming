{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26c324b",
   "metadata": {},
   "source": [
    "# Arquitectura de Streaming en tiempo real. \n",
    "#### Autores: Yahya El Baroudi, Samuel Corrionero, Ismael Gonz√°lez y Jairo Farf√°n. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e99d9",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Introducci√≥n y Objetivos\n",
    "El objetivo de esta pr√°ctica es implementar una arquitectura de **Big Data en Streaming** capaz de ingerir, procesar y analizar datos generados en tiempo real.\n",
    "\n",
    "Hemos simulado un entorno de red social (similar a Twitter/X) para detectar **Trending Topics** (hashtags m√°s populares) cada minuto. Para ello, utilizamos las siguientes tecnolog√≠as:\n",
    "* **Docker:** Para la virtualizaci√≥n de la infraestructura.\n",
    "* **Apache Kafka:** Como broker de mensajer√≠a para desacoplar el productor del consumidor.\n",
    "* **Apache Spark (Structured Streaming):** Para el procesamiento de datos utilizando la API de **Dataframes**, tal y como se especifica en el temario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304feb1",
   "metadata": {},
   "source": [
    "### 2. Infraestructura (Docker)\n",
    "Desplegamos un cl√∫ster de Kafka y Zookeeper utilizando `docker-compose`. Esto garantiza la portabilidad del proyecto y permite levantar los servicios necesarios sin instalaciones complejas en el sistema operativo anfitri√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1e302-7232-4642-bc50-293ec23c3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üèóÔ∏è Levantando infraestructura Docker...\")\n",
    "!docker compose -f docker/docker-compose.yml up -d\n",
    "\n",
    "print(\"‚è≥ Esperando 30 segundos a que Kafka arranque...\")\n",
    "time.sleep(30) # Damos tiempo a que \"caliente\"\n",
    "print(\"‚úÖ Infraestructura lista.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c82c47",
   "metadata": {},
   "source": [
    "### 3. Ingesta de Datos (El Productor)\n",
    "\n",
    "Para simular el flujo de datos, utilizamos un script en Python (`producer.py`) que act√∫a como generador de eventos.\n",
    "\n",
    "* **Funcionamiento:** Genera tweets sint√©ticos en formato JSON con campos aleatorios (usuario, texto, hashtag).\n",
    "* **Ejecuci√≥n en Notebook:** Dado que el productor se ejecuta en un bucle infinito, utilizamos la librer√≠a `subprocess` para lanzarlo en un hilo en segundo plano (background). Esto permite que el Notebook siga disponible para ejecutar las celdas de Spark sin bloquearse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b892e-f4bf-47bb-8ac0-321e60cdeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "\n",
    "print(\"twitter_simulator: üöÄ Iniciando simulador de tweets en segundo plano...\")\n",
    "# Esto lanza el script sin bloquear la celda\n",
    "productor_process = subprocess.Popen([\"python\", \"src/productor.py\"])\n",
    "print(f\"‚úÖ Productor corriendo (PID: {productor_process.pid})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e45f5d",
   "metadata": {},
   "source": [
    "### 4.1. Configuraci√≥n de Librer√≠as y Dependencias\n",
    "\n",
    "En este primer paso, importamos las funciones necesarias de **PySpark SQL** para trabajar con Dataframes.\n",
    "\n",
    "Un punto cr√≠tico aqu√≠ es la configuraci√≥n din√°mica del entorno: detectamos la versi√≥n de Spark instalada y forzamos la descarga del paquete `.jar` (`spark-sql-kafka`) necesario para conectar Spark con Kafka, ya que este conector no viene instalado por defecto. Tambi√©n preparamos el directorio donde guardaremos los resultados (`data/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced5326-a330-4345-9f68-67fb773e64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, current_timestamp, window, count, max as max_\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark\n",
    "\n",
    "# Configuraci√≥n del entorno: Descarga autom√°tica del conector de Kafka\n",
    "spark_version = pyspark.__version__.split(\"+\")[0]\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_2.12:{spark_version} pyspark-shell'\n",
    "\n",
    "# Creamos directorio para persistencia de datos\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2595a",
   "metadata": {},
   "source": [
    "### 4.2. Inicializaci√≥n de la SparkSession\n",
    "\n",
    "La `SparkSession` es el punto de entrada para programar con la API de Dataframes y Datasets.\n",
    "\n",
    "Configuramos el `master` como `local[*]` para utilizar todos los n√∫cleos de la CPU disponibles en la m√°quina. Adem√°s, ajustamos `shuffle.partitions` a 2 (por defecto son 200). Esto es una optimizaci√≥n crucial para entornos locales con pocos datos, ya que evita crear cientos de tareas vac√≠as innecesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar la sesi√≥n de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStreaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reducimos el nivel de log para mantener la salida limpia\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c797db7",
   "metadata": {},
   "source": [
    "### 4.3. Conexi√≥n y Lectura desde Kafka\n",
    "\n",
    "Utilizamos `spark.readStream` para establecer una conexi√≥n continua con el broker de Kafka. Esto crea un *Streaming DataFrame*, que representa una tabla de datos infinita que crece constantemente.\n",
    "\n",
    "* **bootstrap.servers:** Apunta a nuestro contenedor Docker (`localhost:9092`).\n",
    "* **subscribe:** Escucha el t√≥pico `tweets_topic` donde el Productor est√° escribiendo.\n",
    "* **startingOffsets:** Configurado en `latest` para leer solo los datos nuevos y no procesar todo el historial desde el principio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669908d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"tweets_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5e1e8",
   "metadata": {},
   "source": [
    "### 4.4. Estructuraci√≥n de Datos (Schema Enforcement)\n",
    "\n",
    "Kafka env√≠a los datos en formato binario (bytes). Para aplicar la l√≥gica de **Spark Dataframes**, debemos transformar estos bytes en una estructura tabular con columnas tipadas.\n",
    "\n",
    "Definimos un `StructType` que coincide con el JSON generado por nuestro productor (`usuario`, `texto`, `hashtag`, `timestamp`). Usamos la funci√≥n `from_json` para parsear la columna y `select` para aplanar la estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67af13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici√≥n estricta del esquema de datos\n",
    "schema = StructType([\n",
    "    StructField(\"usuario\", StringType(), True),\n",
    "    StructField(\"texto\", StringType(), True),\n",
    "    StructField(\"hashtag_principal\", StringType(), True),\n",
    "    StructField(\"timestamp\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# Transformaci√≥n: De binario -> JSON -> Columnas\n",
    "df_parsed = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .where(col(\"hashtag_principal\").isNotNull()) \\\n",
    "    .withColumn(\"ts\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc97e35",
   "metadata": {},
   "source": [
    "### 4.5. Agregaci√≥n por Ventanas de Tiempo\n",
    "\n",
    "Aqu√≠ reside la l√≥gica principal del Streaming. En lugar de contar hashtags desde el inicio de los tiempos, utilizamos operaciones de ventana (`window`).\n",
    "\n",
    "Agrupamos los datos en intervalos de **60 segundos** bas√°ndonos en la hora de llegada (`ts`). Esto nos permite calcular los *Trending Topics* din√°micamente minuto a minuto. El `watermark` nos ayuda a gestionar datos que llegan con un ligero retraso, descartando aquellos que sean demasiado viejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513add77",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo = df_parsed \\\n",
    "    .withWatermark(\"ts\", \"1 minutes\") \\\n",
    "    .groupBy(window(col(\"ts\"), \"60 seconds\"), col(\"hashtag_principal\")) \\\n",
    "    .agg(count(\"*\").alias(\"total\")) \\\n",
    "    .orderBy(col(\"total\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e235c",
   "metadata": {},
   "source": [
    "### 4.6. Funci√≥n de Salida Personalizada (ForeachBatch)\n",
    "\n",
    "Spark Streaming procesa los datos en micro-lotes (*micro-batches*). Definimos una funci√≥n `mostrar_en_jupyter` que se ejecutar√° cada vez que un micro-lote est√© listo.\n",
    "\n",
    "Esta funci√≥n realiza dos tareas:\n",
    "1.  **Persistencia:** Guarda el lote procesado como un archivo CSV en el disco para auditor√≠a.\n",
    "2.  **Visualizaci√≥n:** Convierte los datos a un DataFrame de Pandas para mostrarlos de forma tabular y est√©tica en el Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_en_jupyter(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Conversi√≥n a Pandas para visualizaci√≥n\n",
    "    pdf = batch_df.limit(10).toPandas()\n",
    "    \n",
    "    print(f\"üìä Actualizaci√≥n del Stream - Batch ID: {batch_id}\")\n",
    "    \n",
    "    # Guardado en disco\n",
    "    csv_path = output_dir / f\"batch_{batch_id}.csv\"\n",
    "    pdf.to_csv(csv_path, index=False)\n",
    "    print(f\"Guardado en {csv_path}\")\n",
    "    \n",
    "    # Impresi√≥n de tabla\n",
    "    print(pdf.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd191a06",
   "metadata": {},
   "source": [
    "### 4.7. Ejecuci√≥n del Query\n",
    "\n",
    "Finalmente, iniciamos el flujo de procesamiento.\n",
    "* **outputMode(\"complete\"):** Indicamos que queremos ver la tabla completa recalculada en cada actualizaci√≥n (necesario para agregaciones con ordenamiento).\n",
    "* **trigger:** Spark intentar√° procesar nuevos datos cada 5 segundos.\n",
    "* **awaitTermination:** Mantiene la celda ejecut√°ndose indefinidamente para escuchar nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9be971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì° Escuchando... La tabla aparecer√° abajo üëá\")\n",
    "\n",
    "query = conteo.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(mostrar_en_jupyter) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# NOTA: Pulsa el bot√≥n \"Stop\" (Cuadrado) en el men√∫ superior para detener la ejecuci√≥n.\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e64d92",
   "metadata": {},
   "source": [
    "### 4.8. Apagamos el productor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932351f-fa2f-4944-b572-59db7f5b55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecuta esto SOLO cuando hayas parado la celda anterior\n",
    "print(\"üõë Apagando el productor...\")\n",
    "productor_process.kill()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19e7a3",
   "metadata": {},
   "source": [
    "### 5. Persistencia y Auditor√≠a\n",
    "\n",
    "Adem√°s de la visualizaci√≥n en tiempo real, el sistema persiste los resultados procesados en disco. Cada lote procesado se guarda como un archivo CSV en la carpeta `data/`.\n",
    "\n",
    "A continuaci√≥n, leemos estos archivos generados para verificar que el hist√≥rico de tendencias se ha almacenado correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242bcca",
   "metadata": {},
   "source": [
    "### üìÇ Visualizaci√≥n de batches grabados\n",
    "Los archivos CSV generados por cada batch se guardan en `data/` con el nombre `batch_<runId>.csv`. La siguiente celda lista esos archivos y los carga con pandas para que puedas inspeccionar los resultados despu√©s de parar el stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üìÇ Batches guardados:\")\n",
    "for path in sorted(Path(\"data\").glob(\"batch_*.csv\")):\n",
    "    print(f\"- {path.name}\")\n",
    "    display(pd.read_csv(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92eb0a",
   "metadata": {},
   "source": [
    "### 5. Desconexi√≥n de infraestructura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"üõë Apagando infraestructura...\")\n",
    "!docker compose -f docker/docker-compose.yml down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arqesp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
