% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{\begin{minipage}{\linewidth}}{\end{minipage}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\author{}
\date{}

\begin{document}

\hypertarget{memoria-del-proyecto-spark-streaming}{%
\section{Memoria del Proyecto: Spark
Streaming}\label{memoria-del-proyecto-spark-streaming}}

\textbf{Título del Proyecto:} Spark Streaming (Simulador en Tiempo Real)

\textbf{Asignatura:} Arquitecturas Especializadas

\textbf{Fecha:} Diciembre 2025

\textbf{Integrantes:}
\begin{itemize}
\item Samuel Corrionero (Arquitecto de Infraestructura)
\item Ismael González Loro (Generador de Datos)
\item Jairo Pabel Farfán Callau (Ingeniero Spark)
\item Yahya El Baroudi El Ouazghari (Documentación)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uxedndice}{%
\subsection{Índice}\label{uxedndice}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{introducciuxf3n}{Introducción}
\item
  \protect\hyperlink{diseuxf1o-y-configuraciuxf3n-de-infraestructura}{Diseño
  y Configuración de Infraestructura}
\item
  \protect\hyperlink{generaciuxf3n-de-datos-producer}{Generación de
  Datos (Producer)}
\item
  \protect\hyperlink{procesamiento-con-spark-consumer}{Procesamiento con
  Spark (Consumer)}
\item
  \protect\hyperlink{documentaciuxf3n}{Documentación}
\item
  \protect\hyperlink{conclusiones}{Conclusiones}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage
\hypertarget{introducciuxf3n}{%
\subsection{Introducción}\label{introducciuxf3n}}

Este proyecto pone en marcha una arquitectura de streaming en tiempo
real: generamos eventos sintéticos parecidos a los de una red social,
los inyectamos en Kafka y los procesamos con Spark sobre una red de
contenedores.

Decidimos apartarnos de los temas propuestos en clase para darle al
proyecto un enfoque más diferente, novedoso y moderno. Quisimos montar
una arquitectura realista basada en contenedores y Big Data, y eso es lo
que se expone en este documento.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage
\hypertarget{diseuxf1o-y-configuraciuxf3n-de-infraestructura}{%
\subsection{Diseño y Configuración de
Infraestructura}\label{diseuxf1o-y-configuraciuxf3n-de-infraestructura}}

\textbf{Responsable:} Samuel Corrionero Fernández

Samuel Corrionero asumió el papel de Arquitecto de Infraestructura. Su
misión fue construir los cimientos sobre los que se apoya todo el
proyecto. En un entorno de Big Data, el código no flota en el vacío;
necesita servidores, redes y sistemas de mensajería robustos.

\hypertarget{quuxe9-es-docker-y-por-quuxe9-lo-usamos}{%
\subsubsection{0. ¿Qué es Docker y por qué lo
usamos?}\label{quuxe9-es-docker-y-por-quuxe9-lo-usamos}}

\hypertarget{concepto-docker-como-muxe1quina-virtual-ligera}{%
\paragraph{Concepto: Docker como ``Máquina Virtual
Ligera''}\label{concepto-docker-como-muxe1quina-virtual-ligera}}

Docker es una herramienta que permite \textbf{empaquetar aplicaciones
con todas sus dependencias} en contenedores aislados. Imagina que Docker
es como una caja de transporte hermética. Dentro de esa caja metemos:
\begin{itemize}
\item La aplicación (Kafka, Zookeeper, Spark)
\item El sistema operativo mínimo que necesita
\item Todas las librerías y configuraciones
\end{itemize}

\textbf{Ventaja:} Esa caja funciona igual en tu PC, en el del compañero,
o en un servidor de producción. No importa si tienes Windows, Mac o
Linux; Docker se encarga de la compatibilidad.

\hypertarget{por-quuxe9-la-usamos-en-este-proyecto}{%
\paragraph{¿Por qué la usamos en este
proyecto?}\label{por-quuxe9-la-usamos-en-este-proyecto}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Reproducibilidad:} Todos los compañeros tienen el mismo
  entorno exacto. No hay confusiones tipo ``a mí me funciona, ¿por qué a
  ti no?''.
\item
  \textbf{Aislamiento:} Kafka y Zookeeper corren en sus propios
  contenedores sin interferir con tu sistema operativo.
\item
  \textbf{Facilidad:} En lugar de instalar Java, descargar Kafka,
  configurar todo manualmente (30 minutos de dolor), ejecutamos un
  comando: \texttt{docker\ compose\ up\ -d}. ¡Listo en 10 segundos!
\item
  \textbf{Escalabilidad:} Si necesitáramos 5 brokers de Kafka en lugar
  de 1, solo cambiaríamos el archivo de configuración. Sin Docker, sería
  una pesadilla.
\end{enumerate}

\hypertarget{arquitectura-de-docker-en-este-proyecto}{%
\paragraph{Arquitectura de Docker en este
Proyecto}\label{arquitectura-de-docker-en-este-proyecto}}

La arquitectura se organiza de la siguiente manera: En tu ordenador
(Host), Docker crea dos contenedores aislados: uno para Zookeeper y otro
para Kafka. Estos dos contenedores están conectados entre sí mediante
una red interna privada de Docker, lo que permite que se comuniquen
directamente entre ellos sin interferencias.

Sin embargo, para que tus aplicaciones Python (que corren en tu máquina
local, fuera de Docker) puedan hablar con Kafka y Zookeeper, estos
contenedores \textbf{exponen puertos hacia el exterior}. Zookeeper
escucha en el puerto \textbf{2181} (desde tu perspectiva, es
\texttt{localhost:2181}) y Kafka en el puerto \textbf{9092}
(\texttt{localhost:9092}).

De esta forma, tu código Python no necesita saber que estas aplicaciones
están dentro de contenedores; simplemente se conecta a
\texttt{localhost:9092} como si estuvieran instaladas directamente en tu
máquina. Docker maneja toda la magia de enrutamiento de red por debajo.

\hypertarget{tecnologuxedas-implementadas-quuxe9-son-y-por-quuxe9-las-usamos}{%
\subsubsection{1. Tecnologías Implementadas (¿Qué son y por qué las
usamos?)}\label{tecnologuxedas-implementadas-quuxe9-son-y-por-quuxe9-las-usamos}}

Para entender la infraestructura, primero debemos definir las piezas
clave que hemos desplegado utilizando contenedores Docker.

\hypertarget{a.-apache-zookeeper-el-coordinador}{%
\paragraph{A. Apache Zookeeper (El
Coordinador)}\label{a.-apache-zookeeper-el-coordinador}}

Imaginemos que Kafka es una gran oficina de correos. \textbf{Zookeeper}
sería el gerente de esa oficina. No reparte cartas (mensajes), pero sabe
quién está trabajando, qué ventanillas están abiertas y mantiene el
orden.
\begin{itemize}
\item \textbf{Función:} Gestiona el clúster, elige al nodo líder y guarda la configuración.
\item \textbf{Necesidad:} Kafka no puede funcionar sin él (en versiones clásicas). Si Zookeeper cae, Kafka pierde el control.
\end{itemize}

\hypertarget{b.-apache-kafka-el-broker-buzuxf3n}{%
\paragraph{B. Apache Kafka (El Broker /
Buzón)}\label{b.-apache-kafka-el-broker-buzuxf3n}}

Es el corazón del sistema de streaming. Funciona como una tubería de
datos de altísima velocidad.
\begin{itemize}
\item \textbf{Concepto clave:} Desacoplamiento. El productor (Python) deja el mensaje en Kafka y se olvida. El consumidor (Spark) lo recoge cuando puede. No necesitan estar conectados directamente.
\item \textbf{Topic:} Es como una ``carpeta'' o ``canal'' dentro de Kafka. Nosotros creamos uno llamado \texttt{tweets\_topic} donde se vuelcan todos los mensajes simulados.
\end{itemize}

\hypertarget{despliegue-con-docker-compose}{%
\subsubsection{2. Despliegue con Docker
Compose}\label{despliegue-con-docker-compose}}

En lugar de instalar Java, Kafka y Zookeeper manualmente en el ordenador
de cada compañero (lo cual suele dar errores de versiones), creamos un
archivo \texttt{docker-compose.yml}. Este archivo es una ``receta'' que
le dice a Docker cómo levantar todo el entorno con un solo comando.

\textbf{Fragmento del código de infraestructura
(\texttt{docker-compose.yml}):}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{services}\KeywordTok{:}
\CommentTok{  \# SERVICIO 1: ZOOKEEPER}
\AttributeTok{  }\FunctionTok{zookeeper}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{image}\KeywordTok{:}\AttributeTok{ confluentinc/cp{-}zookeeper:7.4.4}
\AttributeTok{    }\FunctionTok{container\_name}\KeywordTok{:}\AttributeTok{ zookeeper}
\AttributeTok{    }\FunctionTok{ports}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ }\StringTok{"2181:2181"}
\AttributeTok{    }\FunctionTok{environment}\KeywordTok{:}
\AttributeTok{      }\FunctionTok{ZOOKEEPER\_CLIENT\_PORT}\KeywordTok{:}\AttributeTok{ }\DecValTok{2181}

\CommentTok{  \# SERVICIO 2: KAFKA}
\AttributeTok{  }\FunctionTok{kafka}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{image}\KeywordTok{:}\AttributeTok{ confluentinc/cp{-}kafka:7.4.4}
\AttributeTok{    }\FunctionTok{container\_name}\KeywordTok{:}\AttributeTok{ kafka}
\AttributeTok{    }\FunctionTok{depends\_on}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ zookeeper}\CommentTok{  \# Espera a que el jefe (Zookeeper) esté listo}
\AttributeTok{    }\FunctionTok{ports}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ }\StringTok{"9092:9092"}
\AttributeTok{    }\FunctionTok{environment}\KeywordTok{:}
\CommentTok{      \# Configuración crítica para que Spark (fuera de Docker) pueda hablar con Kafka (dentro de Docker)}
\AttributeTok{      }\FunctionTok{KAFKA\_ADVERTISED\_LISTENERS}\KeywordTok{:}\AttributeTok{ PLAINTEXT://localhost:9092}
\AttributeTok{      }\FunctionTok{KAFKA\_ZOOKEEPER\_CONNECT}\KeywordTok{:}\AttributeTok{ }\StringTok{\textquotesingle{}zookeeper:2181\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\textbf{Explicación técnica:}
\begin{itemize}
\item \textbf{\texttt{depends\_on}}: Asegura que Kafka no arranque hasta que Zookeeper esté listo.
\item \textbf{\texttt{KAFKA\_ADVERTISED\_LISTENERS}}: Este fue el mayor reto. Por defecto, Kafka anuncia su IP interna de Docker. Tuvimos que configurarlo para que anuncie \texttt{localhost:9092}, permitiendo así que los scripts de Python (que corren en el host, fuera de Docker)
\end{itemize}
puedan conectarse.

\hypertarget{comandos-de-gestiuxf3n-y-operaciuxf3n}{%
\subsubsection{3. Comandos de Gestión y
Operación}\label{comandos-de-gestiuxf3n-y-operaciuxf3n}}

Para operar esta infraestructura, se definieron una serie de comandos
que el equipo debía ejecutar.

\textbf{Paso 1: Levantar la infraestructura}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ compose }\AttributeTok{{-}f}\NormalTok{ docker/docker{-}compose.yml up }\AttributeTok{{-}d}
\end{Highlighting}
\end{Shaded}

\emph{El flag \texttt{-d} (detached) permite que los contenedores corran
en segundo plano sin bloquear la terminal.}

\textbf{Paso 2: Verificar el estado}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ ps}
\end{Highlighting}
\end{Shaded}

\emph{Este comando nos confirma que los contenedores \texttt{kafka} y
\texttt{zookeeper} están en estado \texttt{Up}.}

\textbf{Paso 3: Creación del Topic (Canal de datos)} Una vez el sistema
está arriba, hay que crear el canal donde viajarán los datos. Usamos
\texttt{docker\ exec} para ejecutar el comando de creación \emph{dentro}
del contenedor de Kafka:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ exec }\AttributeTok{{-}it}\NormalTok{ kafka kafka{-}topics }\AttributeTok{{-}{-}create} \DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}bootstrap{-}server}\NormalTok{ localhost:9092 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}replication{-}factor}\NormalTok{ 1 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}partitions}\NormalTok{ 1 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}topic}\NormalTok{ tweets\_topic}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{-\/-topic\ tweets\_topic}}: El nombre del canal que
  usarán tanto el Productor como el Consumidor.
\end{itemize}

\textbf{Paso 4: Detener / Apagar la infraestructura} Cuando termines el
desarrollo o quieras limpiar tu máquina, es importante detener los
contenedores. Existen dos formas:

\textbf{Opción A: Detener sin eliminar (mantiene los datos)}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ compose }\AttributeTok{{-}f}\NormalTok{ docker/docker{-}compose.yml stop}
\end{Highlighting}
\end{Shaded}

Los contenedores se pausan pero siguen existiendo. Puedes reanudarlos
con \texttt{docker\ compose\ ...\ start}.

\textbf{Opción B: Detener y eliminar (limpieza total)}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ compose }\AttributeTok{{-}f}\NormalTok{ docker/docker{-}compose.yml down}
\end{Highlighting}
\end{Shaded}

Este comando:
\begin{itemize}
\item Detiene todos los contenedores.
\item Elimina los contenedores (no las imágenes base).
\item Libera la red de Docker.
\end{itemize}

\textbf{Nota importante:} Si utilizas volúmenes persistentes (datos
almacenados en disco), agrégale el flag para eliminarlos también:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ compose }\AttributeTok{{-}f}\NormalTok{ docker/docker{-}compose.yml down }\AttributeTok{{-}v}
\end{Highlighting}
\end{Shaded}

\textbf{Verificar que todo se apagó:}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ ps}
\end{Highlighting}
\end{Shaded}

Debería mostrar una lista vacía (sin contenedores corriendo).

\hypertarget{validaciuxf3n-de-infraestructura-testing}{%
\subsubsection{4. Validación de Infraestructura
(Testing)}\label{validaciuxf3n-de-infraestructura-testing}}

Para asegurar que todo funcionaba correctamente antes de que el resto
del equipo empezara a desarrollar, Samuel creó un \textbf{script de
prueba} (\texttt{tests/tester.py}). Este script actúa como un simulador
de ``salud del sistema''.

\textbf{¿Qué hace el Tester?}
\begin{enumerate}
\item Actúa como \textbf{Productor}: Envía 5 mensajes de prueba a \texttt{tweets\_topic}.
\item Actúa como \textbf{Consumidor}: Lee esos mismos mensajes para confirmar que todo fluye correctamente.
\end{enumerate}

\textbf{El código (versión simplificada):}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ kafka }\ImportTok{import}\NormalTok{ KafkaProducer, KafkaConsumer}
\ImportTok{import}\NormalTok{ json}

\CommentTok{\# Paso 1: PRODUCTOR {-} Enviar datos}
\NormalTok{producer }\OperatorTok{=}\NormalTok{ KafkaProducer(}
\NormalTok{    bootstrap\_servers}\OperatorTok{=}\StringTok{\textquotesingle{}localhost:9092\textquotesingle{}}\NormalTok{,}
\NormalTok{    value\_serializer}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ v: json.dumps(v).encode(}\StringTok{\textquotesingle{}utf{-}8\textquotesingle{}}\NormalTok{)}
\NormalTok{)}

\NormalTok{mensaje }\OperatorTok{=}\NormalTok{ \{}\StringTok{"usuario"}\NormalTok{: }\StringTok{"User\_1"}\NormalTok{, }\StringTok{"contenido"}\NormalTok{: }\StringTok{"Hola Kafka"}\NormalTok{\}}
\NormalTok{producer.send(}\StringTok{\textquotesingle{}tweets\_topic\textquotesingle{}}\NormalTok{, value}\OperatorTok{=}\NormalTok{mensaje)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"[OK] Enviado: }\SpecialCharTok{\{}\NormalTok{mensaje}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Paso 2: CONSUMIDOR {-} Leer datos}
\NormalTok{consumer }\OperatorTok{=}\NormalTok{ KafkaConsumer(}
    \StringTok{\textquotesingle{}tweets\_topic\textquotesingle{}}\NormalTok{,}
\NormalTok{    bootstrap\_servers}\OperatorTok{=}\StringTok{\textquotesingle{}localhost:9092\textquotesingle{}}\NormalTok{,}
\NormalTok{    auto\_offset\_reset}\OperatorTok{=}\StringTok{\textquotesingle{}earliest\textquotesingle{}}\NormalTok{,}
\NormalTok{    value\_deserializer}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: json.loads(x.decode(}\StringTok{\textquotesingle{}utf{-}8\textquotesingle{}}\NormalTok{))}
\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ mensaje }\KeywordTok{in}\NormalTok{ consumer:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"[OK] Recibido: }\SpecialCharTok{\{}\NormalTok{mensaje}\SpecialCharTok{.}\NormalTok{value}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Cómo ejecutar el test:}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Asegúrate de que Docker está corriendo}
\ExtensionTok{docker}\NormalTok{ ps}

\CommentTok{\# 2. Ejecuta el tester}
\ExtensionTok{python}\NormalTok{ tests/tester.py}
\end{Highlighting}
\end{Shaded}

\textbf{Salida esperada si TODO funciona:}

\begin{verbatim}
>>> 1. INICIANDO PRODUCTOR (Enviando mensajes a localhost:9092)...
 [OK] Enviado: {'usuario': 'User_1', 'mensaje': 'Hola Kafka', 'contador': 1}
 [OK] Enviado: {'usuario': 'User_2', 'mensaje': 'Hola Kafka', 'contador': 2}
 ...
>>> 2. INICIANDO CONSUMIDOR (Leyendo de tweets_topic)...
 [OK] Recibido: {'usuario': 'User_1', 'mensaje': 'Hola Kafka', 'contador': 1}
 [OK] Recibido: {'usuario': 'User_2', 'mensaje': 'Hola Kafka', 'contador': 2}
 ...
>>> Consumidor finalizado con éxito. ¡TODO FUNCIONA!
\end{verbatim}

\textbf{¿Qué demuestra este test?}
\begin{itemize}
\item [OK] Docker está levantado y Kafka escucha en \texttt{localhost:9092}.
\item [OK] El topic \texttt{tweets\_topic} existe y acepta mensajes.
\item [OK] La infraestructura es capaz de desacoplar productor-consumidor (el patrón fundamental del streaming).
\item [OK] Los datos fluyen correctamente desde Python hacia Kafka.
\end{itemize}

Este test fue crítico para identificar problemas de configuración antes
de empezar el trabajo real de Spark.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage
\hypertarget{generaciuxf3n-de-datos-producer}{%
\subsection{Generación de Datos
(Producer)}\label{generaciuxf3n-de-datos-producer}}

\textbf{Responsable:} Ismael González Loro

Tras establecer la infraestructura base (Docker y Kafka), el siguiente
reto fue alimentar el sistema. En una arquitectura Big Data, el
``pipeline'' o tubería de procesamiento no sirve de nada si no tiene un
flujo de entrada constante.

Mi misión como \textbf{Responsable de la Ingesta} fue crear la
``realidad simulada'': un generador de tráfico que emule el
comportamiento de usuarios de una red social (tipo Twitter/X) en tiempo
real, garantizando que el clúster de Spark siempre tenga datos que
procesar.

\hypertarget{estrategia-por-quuxe9-un-simulador-y-no-la-api-real}{%
\subsubsection{2.1. Estrategia: ¿Por qué un simulador y no la API
real?}\label{estrategia-por-quuxe9-un-simulador-y-no-la-api-real}}

Inicialmente se valoró conectar con la API oficial de Twitter (X). Sin
embargo, se optó por desarrollar un \textbf{Generador Sintético
(Synthetic Data Generator)} por tres razones técnicas y operativas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Independencia y Resiliencia:} Las APIs públicas tienen límites
  de tasa (\emph{rate-limits}) y costes asociados. Un simulador nos
  permite generar tráfico infinito sin bloqueos, asegurando que la
  demostración en vivo no falle por causas externas.
\item
  \textbf{Control del Dato:} Para probar las capacidades de Spark
  Streaming (agrupaciones y ventanas de tiempo), necesitábamos asegurar
  que ciertos hashtags (ej: \texttt{\#Spark}) aparecieran con más
  frecuencia que otros. Un generador sintético nos permite ``trucar''
  las probabilidades para hacer la demo más visual.
\item
  \textbf{Velocidad Ajustable:} Podemos acelerar o frenar el flujo de
  datos modificando una simple variable de tiempo (\texttt{sleep}), algo
  imposible con datos reales.
\end{enumerate}

\hypertarget{diseuxf1o-tuxe9cnico-del-productor}{%
\subsubsection{2.2. Diseño Técnico del
Productor}\label{diseuxf1o-tuxe9cnico-del-productor}}

El componente desarrollado es un script en Python (\texttt{producer.py})
que implementa el patrón de diseño \textbf{Producer} de Kafka.

\hypertarget{a.-libreruxedas-y-conexiuxf3n}{%
\paragraph{A. Librerías y
Conexión}\label{a.-libreruxedas-y-conexiuxf3n}}

Se utilizó la librería \texttt{kafka-python} por su robustez y
simplicidad. El script se conecta al ``Broker'' expuesto por Docker en
\texttt{localhost:9092}.

Una decisión clave fue la \textbf{Serialización}. Kafka transmite bytes
puros, por lo que el productor se encarga de transformar nuestros
objetos Python (diccionarios) a formato JSON y codificarlos en UTF-8
antes del envío.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Configuración del Serializador en el Producer}
\NormalTok{producer }\OperatorTok{=}\NormalTok{ KafkaProducer(}
\NormalTok{    bootstrap\_servers}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}localhost:9092\textquotesingle{}}\NormalTok{],}
\NormalTok{    value\_serializer}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: json.dumps(x).encode(}\StringTok{\textquotesingle{}utf{-}8\textquotesingle{}}\NormalTok{) }\CommentTok{\# Diccionario {-}\textgreater{} JSON Bytes}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{b.-estructura-del-dato-schema}{%
\paragraph{B. Estructura del Dato
(Schema)}\label{b.-estructura-del-dato-schema}}

Para facilitar el trabajo del \textbf{Consumidor (Persona C)}, diseñé un
esquema de datos JSON limpio pero enriquecido. En lugar de enviar solo
texto plano, enviamos una estructura estructurada:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{usuario}}: Simula quién escribe.
\item
  \textbf{\texttt{texto}}: La frase completa (ej: ``estoy aprendiendo
  mucho con \#Spark'').
\item
  \textbf{\texttt{hashtag\_principal}}: Aquí apliqué una lógica de
  pre-procesamiento. Aunque el hashtag está en el texto, lo envío
  también en un campo separado. Esto permite a Spark hacer el
  \texttt{groupBy} directamente sin necesidad de expresiones regulares
  complejas, optimizando el rendimiento.
\item
  \textbf{\texttt{timestamp}}: Marca de tiempo para posibles análisis de
  latencia.
\end{itemize}

\hypertarget{implementaciuxf3n-del-algoritmo}{%
\subsubsection{2.3. Implementación del
Algoritmo}\label{implementaciuxf3n-del-algoritmo}}

El núcleo del generador reside en un bucle infinito que construye
mensajes aleatorios mediante listas predefinidas.

\textbf{Fragmento destacado del código (\texttt{producer.py}):}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ generador\_tweets():}
    \CommentTok{\# Listas de componentes aleatorios}
\NormalTok{    usuarios }\OperatorTok{=}\NormalTok{ [}\StringTok{"@DataFan"}\NormalTok{, }\StringTok{"@SparkGuru"}\NormalTok{, }\StringTok{"@PythonDev"}\NormalTok{]}
\NormalTok{    hashtags }\OperatorTok{=}\NormalTok{ [}\StringTok{"\#Spark"}\NormalTok{, }\StringTok{"\#BigData"}\NormalTok{, }\StringTok{"\#RealTime"}\NormalTok{, }\StringTok{"\#IA"}\NormalTok{]}
\NormalTok{    frases }\OperatorTok{=}\NormalTok{ [}\StringTok{"increíble la velocidad de"}\NormalTok{, }\StringTok{"mañana examen de"}\NormalTok{, }\StringTok{"proyecto sobre"}\NormalTok{]}
    
    \CommentTok{\# Selección aleatoria (Randomness)}
\NormalTok{    usuario }\OperatorTok{=}\NormalTok{ random.choice(usuarios)}
\NormalTok{    hashtag }\OperatorTok{=}\NormalTok{ random.choice(hashtags)}
\NormalTok{    frase }\OperatorTok{=}\NormalTok{ random.choice(frases)}
    
    \CommentTok{\# Construcción del objeto JSON}
    \ControlFlowTok{return}\NormalTok{ \{}
        \StringTok{"usuario"}\NormalTok{: usuario,}
        \StringTok{"texto"}\NormalTok{: }\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{frase}\SpecialCharTok{\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{hashtag}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{,}
        \StringTok{"hashtag\_principal"}\NormalTok{: hashtag, }\CommentTok{\# Campo optimizado para Spark}
        \StringTok{"timestamp"}\NormalTok{: time.time()}
\NormalTok{    \}}

\CommentTok{\# Bucle de emisión}
\ControlFlowTok{while} \VariableTok{True}\NormalTok{:}
\NormalTok{    tweet }\OperatorTok{=}\NormalTok{ generador\_tweets()}
\NormalTok{    producer.send(}\StringTok{\textquotesingle{}tweets\_topic\textquotesingle{}}\NormalTok{, value}\OperatorTok{=}\NormalTok{tweet)}
\NormalTok{    time.sleep(}\DecValTok{1}\NormalTok{) }\CommentTok{\# Simulación de ritmo humano (1 tweet/seg)}
\end{Highlighting}
\end{Shaded}

\hypertarget{validaciuxf3n-y-pruebas-de-integraciuxf3n}{%
\subsubsection{2.4. Validación y Pruebas de
Integración}\label{validaciuxf3n-y-pruebas-de-integraciuxf3n}}

Antes de entregar el testigo a la fase de procesamiento, verifiqué la
correcta inyección de datos en el bus de mensajería.

Al no tener aún el consumidor de Spark listo, utilicé las herramientas
nativas de Kafka dentro del contenedor Docker para ``espiar'' el topic
\texttt{tweets\_topic} y confirmar la llegada de los JSONs.

\textbf{Comando de validación utilizado:}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ exec }\AttributeTok{{-}it}\NormalTok{ kafka kafka{-}console{-}consumer }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}bootstrap{-}server}\NormalTok{ localhost:9092 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}topic}\NormalTok{ tweets\_topic }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}from{-}beginning}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Conclusión de la Fase B:} El sistema de ingesta es operativo y
autónomo. Genera un flujo continuo de datos estructurados, desacoplando
la generación del procesamiento y cumpliendo con los requisitos de la
arquitectura Lambda propuesta.

\newpage
\hypertarget{procesamiento-con-spark-consumer}{%
\subsection{Procesamiento con Spark
(Consumer)}\label{procesamiento-con-spark-consumer}}

\textbf{Responsable:} Jairo Pabel Farfán Callau

El rol del consumidor es transformar el flujo bruto de mensajes de Kafka
en información agregada sobre \emph{trending topics} en tiempo (casi)
real. Mientras que el productor simula una ``red social'' generando
tweets con hashtags, el consumidor con Spark Structured Streaming se
encarga de:

\begin{itemize}
\item
  Leer continuamente esos mensajes desde Kafka.
\item
  Parsear el JSON para extraer el hashtag\_principal.
\item
  Contar apariciones por hashtag en ventanas de tiempo.
\item
  Mostrar cada pocos segundos un ranking actualizado de los hashtags más
  usados.
\end{itemize}

Todo esto se implementa en el script src/spark\_consumer.py, utilizando
\textbf{PySpark} y el conector oficial \textbf{spark-sql-kafka-0-10}.

\hypertarget{objetivo-del-consumidor}{%
\subsubsection{1. Objetivo del
consumidor}\label{objetivo-del-consumidor}}

El objetivo principal del consumidor es \textbf{calcular y mostrar los
hashtags más utilizados en la última ventana de tiempo}, imitando el
funcionamiento de un sistema de ``trending topics'':

\begin{itemize}
\item
  Se define una \textbf{ventana fija de 60 segundos} (un minuto
  ``simulado'' de red social).
\item
  Cada \textbf{10 segundos} se recalculan los conteos de hashtags de ese
  minuto.
\item
  La salida es una tabla ordenada de mayor a menor número de
  apariciones.
\end{itemize}

De esta forma se consigue un compromiso razonable entre:

\begin{itemize}
\item
  \emph{Estabilidad} del ranking (se mantiene el histórico de un minuto
  completo).
\item
  \emph{Reactividad} (la tabla se actualiza cada 10 segundos).
\end{itemize}

\hypertarget{lectura-del-flujo-desde-kafka}{%
\subsubsection{2. Lectura del flujo desde
Kafka}\label{lectura-del-flujo-desde-kafka}}

Para consumir los mensajes, se inicializa una sesión de Spark y se
configura una \textbf{fuente de streaming Kafka} apuntando al mismo
topic que usa el productor (tweets\_topic):

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{raw\_df = (}
\AttributeTok{    spark.readStream}
\AttributeTok{        .format("kafka")}
\AttributeTok{        .option("kafka.bootstrap.servers", "localhost:9092")}
\AttributeTok{        .option("subscribe", "tweets\_topic")}
\AttributeTok{        .option("startingOffsets", "latest")}
\AttributeTok{        .load()}
\AttributeTok{)}
\end{Highlighting}
\end{Shaded}

Puntos clave:

\begin{itemize}
\item
  startingOffsets = ``latest'' indica que el consumidor solo procesa
  \textbf{mensajes nuevos} a partir del momento en que se arranca, sin
  re-leer históricos anteriores.
\item
  La columna value llega en binario, por lo que se castea a STRING para
  poder parsear el JSON enviado por el productor.
\end{itemize}

\hypertarget{modelo-de-datos-y-parseo-del-json}{%
\subsubsection{3. Modelo de datos y parseo del
JSON}\label{modelo-de-datos-y-parseo-del-json}}

El productor envía mensajes JSON con esta estructura simplificada:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{\{}
\AttributeTok{  }\FunctionTok{"usuario"}\KeywordTok{:}\AttributeTok{ }\StringTok{"User\_X"}\KeywordTok{,}
\AttributeTok{  }\FunctionTok{"texto"}\KeywordTok{:}\AttributeTok{ }\StringTok{"mensaje de ejemplo"}\KeywordTok{,}
\AttributeTok{  }\FunctionTok{"hashtag\_principal"}\KeywordTok{:}\AttributeTok{ }\StringTok{"\#BigData"}\KeywordTok{,}
\AttributeTok{  }\FunctionTok{"timestamp"}\KeywordTok{:}\AttributeTok{ }\FloatTok{1733940000.0}
\KeywordTok{\}}
\end{Highlighting}
\end{Shaded}

En el consumidor se define un \textbf{schema explícito} en PySpark y se
realiza el parseo:

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{schema = StructType([}
\AttributeTok{    StructField("usuario", StringType(), True),}
\AttributeTok{    StructField("texto", StringType(), True),}
\AttributeTok{    StructField("hashtag\_principal", StringType(), True),}
\AttributeTok{    StructField("timestamp", DoubleType(), True),}
\AttributeTok{])}


\AttributeTok{value\_df = raw\_df.selectExpr("CAST(value AS STRING) as json\_str")}


\AttributeTok{parsed\_df = (}
\AttributeTok{    value\_df}
\AttributeTok{    .select(from\_json(col("json\_str"), schema).alias("data"))}
\AttributeTok{    .select("data.*")}
\AttributeTok{)}
\end{Highlighting}
\end{Shaded}

Después se limpian posibles valores nulos y se añade una marca de tiempo
de procesamiento que servirá para las ventanas temporales:

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{df\_with\_ts = (}
\AttributeTok{    parsed\_df}
\AttributeTok{    .where(col("hashtag\_principal").isNotNull())}
\AttributeTok{    .withColumn("ts", current\_timestamp())}
\AttributeTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ventanas-temporales-y-luxf3gica-de-agregaciuxf3n}{%
\subsubsection{4. Ventanas temporales y lógica de
agregación}\label{ventanas-temporales-y-luxf3gica-de-agregaciuxf3n}}

El núcleo del procesamiento consiste en agrupar los mensajes por hashtag
dentro de ventanas temporales:

\begin{itemize}
\item
  \textbf{Ventana fija de 60 segundos}, alineada a minutos naturales
  (HH:MM:00,HH:MM:59HH:MM:00, HH:MM:59HH:MM:00,HH:MM:59).
\item
  \textbf{Watermark de 60 segundos} para descartar datos demasiado
  atrasados y controlar el estado interno de Spark.
\item
  Conteo de mensajes por cada hashtag\_principal.
\end{itemize}

La definición de la ventana es:

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{windowed\_counts = (}
\AttributeTok{    df\_with\_ts}
\AttributeTok{    .withWatermark("ts", "60 seconds")}
\AttributeTok{    .groupBy(}
\AttributeTok{        window(col("ts"), "60 seconds"),}
\AttributeTok{        col("hashtag\_principal")}
\AttributeTok{    )}
\AttributeTok{    .agg(count("*").alias("num\_ocurrencias"))}
\AttributeTok{)}
\end{Highlighting}
\end{Shaded}

A partir de ahí, se utiliza foreachBatch para procesar cada micro-batch
de forma ``estática'', ordenar y limitar los resultados:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{def process\_batch(batch\_df, batch\_id)}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{if batch\_df.isEmpty()}\KeywordTok{:}
\AttributeTok{        return}


\CommentTok{    \# Ventana más reciente disponible en este batch}
\AttributeTok{    max\_end\_row = batch\_df.agg(max\_("window.end").alias("max\_end")).collect()[0]}
\AttributeTok{    max\_end = max\_end\_row["max\_end"]}


\AttributeTok{    }\FunctionTok{if max\_end is None}\KeywordTok{:}
\AttributeTok{        return}


\AttributeTok{    latest\_window\_df = batch\_df.filter(col("window.end") == max\_end)}


\AttributeTok{    top\_hashtags = (}
\AttributeTok{        latest\_window\_df}
\AttributeTok{        .orderBy(col("num\_ocurrencias").desc())}
\AttributeTok{    )}


\AttributeTok{    print("\textbackslash{}n========================================")}
\AttributeTok{    print(f"Batch \{batch\_id\} {-} Snapshot en \{datetime.now().strftime(\textquotesingle{}\%Y{-}\%m{-}\%d \%H:\%M:\%S\textquotesingle{})\}")}
\AttributeTok{    print(f"Ventana del minuto [\{max\_end {-} timedelta(seconds=60)\} , \{max\_end\}]")}
\AttributeTok{    print("Top hashtags en este minuto:")}
\AttributeTok{    top\_hashtags.show(truncate=False)}
\AttributeTok{    print("========================================\textbackslash{}n")}
\AttributeTok{  }
\end{Highlighting}
\end{Shaded}

Por último, se arranca el streaming con un \textbf{trigger de 10
segundos}:

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{query = (}
\AttributeTok{    windowed\_counts}
\AttributeTok{    .writeStream}
\AttributeTok{    .outputMode("update")}
\AttributeTok{    .foreachBatch(process\_batch)}
\AttributeTok{    .trigger(processingTime="10 seconds")}
\AttributeTok{    .start()}
\AttributeTok{)}


\AttributeTok{query.awaitTermination()}
\end{Highlighting}
\end{Shaded}

Esto implica que cada \textasciitilde10 segundos se recibe un nuevo lote
(\emph{batch}) con las actualizaciones y se imprime un nuevo snapshot
del ranking.

\hypertarget{formato-de-salida-e-interpretaciuxf3n}{%
\subsubsection{5. Formato de salida e
interpretación}\label{formato-de-salida-e-interpretaciuxf3n}}

En ejecución, el consumidor genera bloques de salida de este estilo:

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{========================================}
\AttributeTok{Batch 12 {-} Snapshot en 2025{-}12{-}16 00:31:20}
\AttributeTok{Ventana del minuto [2025{-}12{-}16 00:30:20 , 2025{-}12{-}16 00:31:20]}
\FunctionTok{Top hashtags en este minuto}\KeywordTok{:}
\AttributeTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\AttributeTok{|window             |hashtag\_principal|num\_ocurrencias|}
\AttributeTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\AttributeTok{|\{...\}              |}\CommentTok{\#BigData         |42             |}
\AttributeTok{|\{...\}              |}\CommentTok{\#Spark           |35             |}
\AttributeTok{|\{...\}              |}\CommentTok{\#Kafka           |18             |}
\AttributeTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\AttributeTok{========================================}
\end{Highlighting}
\end{Shaded}

Interpretación:

\begin{itemize}
\item
  \textbf{Batch N}: número de actualización desde que se arrancó el
  consumidor.
\item
  \textbf{Snapshot en \ldots{}}: momento en el que se ha generado ese
  resumen.
\item
  \textbf{Ventana del minuto t0,t1t0, t1t0,t1}: intervalo de 60 segundos
  que se está analizando.
\item
  La tabla muestra, para esa ventana de un minuto, cuántas veces ha
  aparecido cada hashtag (num\_ocurrencias), ordenados de mayor a menor.
\end{itemize}

En términos conceptuales, esto equivale a un ``trending topics del
último minuto'', recalculado y reimpreso de forma periódicacada 10
segundos.

\newpage
\hypertarget{documentaciuxf3n}{%
\subsection{Documentación}\label{documentaciuxf3n}}

\textbf{Responsable:} Yahya El Baroudi El Ouazghari

Yahya El Baroudi El Ouazghari ha asumido el rol de Responsable de
Documentación e Integración. Como responsable de la documentación, el
objetivo principal ha sido traducir la complejidad técnica de la
infraestructura (Docker) y el código (Python/Spark) en un entregable
claro, reproducible y educativo. En un proyecto de Big Data, la
documentación actúa como el puente que permite que el trabajo técnico
sea comprendido y evaluado correctamente, cumpliendo con los criterios
de evaluación que otorgan un gran peso a este apartado.

Para cumplir con los requisitos de la asignatura, se ha diseñado una
estrategia de presentación híbrida y se ha elaborado tanto la guía de
ejecución como el material de defensa.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Estrategia Híbrida: Scripts vs.~Notebooks} Uno de los desafíos
  principales ha consistido en adaptar una arquitectura de streaming
  real (que habitualmente funciona con scripts .py en servidores) al
  formato académico solicitado.
\end{enumerate}

Según los requerimientos de la asignatura, se exigía un ``Caso práctico
comentado en detalle utilizando Google Colab o Jupyter''. Sin embargo,
ejecutar servicios de infraestructura como Kafka dentro de un entorno de
celdas (Notebook) presenta problemas de bloqueo en los bucles de
ejecución.

Para solucionar esta problemática, se ha estructurado el proyecto en dos
niveles:

\begin{itemize}
\item
  \textbf{Nivel de Producción} (src/): Se mantiene el código limpio en
  archivos .py (producer.py y consumer.py), demostrando capacidad de
  desarrollo de software profesional.
\item
  \textbf{Nivel de Presentación} (Notebook.ipynb): Se ha creado un
  ``Notebook Maestro'' que orquesta todo el proyecto. Este cuaderno no
  solo contiene código, sino que utiliza celdas Markdown para explicar
  paso a paso la teoría detrás de cada bloque, actuando como una memoria
  interactiva ejecutable.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Implementación del Notebook Maestro} El Notebook entregable
  integra las tres partes del proyecto en un solo flujo visual. Se han
  implementado soluciones técnicas específicas para hacer esto posible:
\end{enumerate}

\begin{itemize}
\item
  \textbf{Orquestación desde Jupyter}: Se utilizan comandos mágicos
  (!docker compose) para controlar la infraestructura desarrollada por
  Samuel directamente desde el navegador.
\item
  \textbf{Ejecución en Segundo Plano (Background)}: Para el Generador de
  Datos de Ismael, se implementó el uso de la librería subprocess. Esto
  permite lanzar el productor en un hilo paralelo sin bloquear la celda
  de ejecución, posibilitando que el notebook continúe hacia la sección
  de Spark.
\item
  \textbf{Visualización en Vivo}: Para el consumidor de Jairo, se
  configuró una salida visual utilizando IPython.display y Pandas. En
  lugar de imprimir texto plano infinito en la consola, el notebook
  muestra una tabla HTML dinámica que se actualiza y refresca cada 5
  segundos con los nuevos Trending Topics, ofreciendo una experiencia de
  usuario superior.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Guía de Despliegue (Manual de Usuario)} Se ha elaborado el
  archivo README.md del repositorio, el cual sirve como manual de
  instrucciones para cualquier persona que desee replicar el proyecto.
  La guía se resume en tres pasos críticos para garantizar la
  reproducibilidad:
\end{enumerate}

\begin{itemize}
\item
  \textbf{Prerrequisitos}: Instalación de Docker Desktop y Anaconda, así
  como la creación del entorno virtual arqesp para aislar las librerías
  pyspark y kafka-python.
\item
  \textbf{Orden de Ejecución} (Start-up Sequence): Se documentó la
  importancia estricta del orden de encendido:

  \begin{itemize}
  \item
    \textbf{1º Infraestructura} (Docker) -\textgreater{} Espera de 30s
    de ``calentamiento''.
  \item
    \textbf{2º Productor} (Generar datos).
  \item
    \textbf{3º Consumidor} (Procesar datos).
  \end{itemize}
\item
  \textbf{Gestión de Errores}: Se incluyó una sección de
  ``Troubleshooting'' en la documentación para resolver problemas
  comunes, tales como la falta de memoria en Docker o conflictos de
  puertos con servicios de Windows.
\end{itemize}

Gracias a esta documentación, el proyecto no es solo un código funcional
en un entorno local específico, sino un sistema robusto capaz de ser
desplegado y evaluado en cualquier máquina con Docker instalado.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Elaboración del Material de Defensa (Presentación)} Para
  cumplir con el requerimiento de ``Defensa pública mediante
  presentación'' especificado en la guía docente, se ha diseñado y
  estructurado el material visual de apoyo.
\end{enumerate}

Esta presentación ha sido elaborada con un enfoque ejecutivo,
sintetizando las horas de trabajo técnico en una exposición clara. El
documento de diapositivas incluye:

\begin{itemize}
\item
  \textbf{Diagramas de Arquitectura}: Simplificación visual de la
  comunicación entre Docker, Kafka y Spark.
\item
  \textbf{Justificación Tecnológica}: Explicación comparativa de por qué
  se eligió Streaming frente a procesamiento Batch tradicional.
\item
  \textbf{Evidencias de Funcionamiento}: Capturas de pantalla del
  sistema en operación para respaldar la demostración en vivo ante
  posibles fallos del directo (``Efecto Demo'').
\end{itemize}

De esta forma, se entrega no solo la documentación técnica exhaustiva
(Notebook y código fuente), sino también un soporte visual adecuado para
la comunicación efectiva de los resultados.

\newpage
\hypertarget{conclusiones}{%
\subsection{Conclusiones}\label{conclusiones}}

Este proyecto ha permitido simular con éxito un entorno de Big Data en
tiempo real, integrando tecnologías punteras como \textbf{Docker},
\textbf{Kafka} y \textbf{Apache Spark}.

A diferencia de los enfoques tradicionales de procesamiento por lotes
(Batch) vistos en clase (como Hive o Pig), esta arquitectura
\textbf{Streaming} permite:
\begin{enumerate}
\item \textbf{Inmediatez:} Los datos se procesan conforme llegan, permitiendo reacciones al instante.
\item \textbf{Desacoplamiento:} Kafka actúa como un buffer robusto que separa la generación de datos de su consumo, evitando cuellos de botella.
\item \textbf{Escalabilidad:} El uso de contenedores Docker facilita el despliegue y la escalabilidad horizontal de los servicios.
\end{enumerate}

En resumen, hemos logrado construir un pipeline completo de datos
(``End-to-End'') que no solo cumple con los requisitos académicos, sino
que se acerca a las arquitecturas reales utilizadas en la industria para
el análisis de redes sociales y eventos en vivo.

\end{document}
