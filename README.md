# ğŸš€ Proyecto Big Data: Simulador de Streaming (Kafka + Spark)

Este proyecto implementa una arquitectura de **Big Data en Tiempo Real**. Simulamos un flujo de datos de una red social (tipo Twitter/X) para procesar tendencias (Trending Topics) al instante.

La arquitectura se basa en un **ClÃºster de un solo nodo (Single-Node Cluster)** virtualizado con Docker.

-----

## ğŸ“‹ Estructura del Proyecto

Antes de tocar nada, entiende quÃ© es cada carpeta y archivo:

  * **`docker/docker-compose.yml`**: ğŸ—ï¸ **Infraestructura.** Define los servicios Zookeeper y Kafka y cÃ³mo se conectan. Docker lee este archivo para levantar toda la infraestructura automÃ¡ticamente.
  * **`src/producer/`**: ğŸ“¤ **Productor.** CÃ³digo fuente para simular el envÃ­o de mensajes (tweets).
  * **`src/consumer/`**: ğŸ“¥ **Consumidor.** CÃ³digo fuente para procesar los mensajes (Spark, etc.).
  * **`src/utils/`**: ğŸ› ï¸ **Utilidades.** Funciones auxiliares y configuraciÃ³n.
  * **`tests/tester.py`**: ğŸ§ª **Test.** Script de prueba para verificar la conexiÃ³n con Kafka.
  * **`data/`**: ğŸ“ **Datos.** Guarda ejemplos pequeÃ±os, dumps o logs que uses para pruebas locales.
  * **`requirements.txt`**: ğŸ“¦ **Dependencias versionadas.** La lista que usamos dentro del entorno `arqesp`.
  * **`.gitignore`**: ğŸ—‘ï¸ **Filtro.** Archivos ignorados por Git.
  * **`README.md`**: ğŸ“– **DocumentaciÃ³n principal.**

-----

## ğŸŒ³ Ãrbol de directorios

```text
AE_spark-streaming/
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ tester.py
â”œâ”€â”€ data/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

-----

## ğŸ› ï¸ Requisitos Previos

Necesitas tener instalado en tu mÃ¡quina:

1.  **Docker & Docker Compose**: El motor que ejecutarÃ¡ los servidores.
2.  **Python 3.9+**: Recomendamos usar **Anaconda/Miniconda**.
3.  **Git**: Para descargar este cÃ³digo.

-----

## ğŸš€ InstalaciÃ³n y Puesta en Marcha

Sigue estos pasos en orden exacto.

### 1\. Clonar el repositorio

Descarga el cÃ³digo a tu mÃ¡quina:

```bash
git clone <URL_DEL_REPOSITORIO>
cd spark-streaming-project
```

### 2\. Preparar el entorno Python

Vamos a crear un entorno limpio para no mezclar librerÃ­as (es recomendable usar conda, pero no es necesario).

```bash
# Crear entorno llamado 'arqesp'
conda create --name arqesp python=3.9 -y

# Activar el entorno
conda activate arqesp

# Instalar las dependencias versionadas del proyecto
pip install -r requirements.txt
```

AdemÃ¡s asegÃºrate de que Java 11 o 17 estÃ© instalada y disponible en `PATH`, ya que PySpark necesita la JVM para arrancar.

### 3\. Levantar la Infraestructura (Docker)

Este comando descargarÃ¡ las imÃ¡genes y encenderÃ¡ Zookeeper y Kafka en segundo plano.

```bash
# Si estÃ¡s en Linux/Mac y requiere permisos, usa 'sudo' delante
cd docker
sudo docker compose up -d
```

*Espera unos segundos hasta que diga "Started" o "Running".*

### 4\. Crear el Canal de ComunicaciÃ³n (Topic)

**âš ï¸ IMPORTANTE:** Este paso solo es necesario hacerlo **una vez** (la primera vez que arrancas el sistema). Creamos el "buzÃ³n" donde se guardarÃ¡n los tweets.

```bash
sudo docker compose exec kafka kafka-topics --create --topic tweets_topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1
```

*Si sale bien, dirÃ¡: `Created topic tweets_topic`.*

-----

## âœ… Verificar que todo funciona

Para asegurarte de que tu ordenador puede hablar con el Kafka que vive dentro de Docker, hemos creado un script de prueba.

AsegÃºrate de tener el entorno activado (`conda activate arqesp`) y ejecuta:

```bash
python tests/tester.py
```

Si ves mensajes con **[âœ”] Enviado** y **[âœ”] Recibido**, Â¡felicidades\! Tu entorno estÃ¡ listo para empezar a desarrollar.

-----

## â„¹ï¸ Datos TÃ©cnicos (Para configuraciÃ³n)

Si necesitas configurar tus scripts (Producer o Spark), usa estos datos:

  * **Servidor Kafka (Bootstrap Server):** `localhost:9092`
  * **Nombre del Topic:** `tweets_topic`
  * **Zookeeper (Interno):** Puerto 2181

-----

## ğŸ›‘ CÃ³mo detener todo

Cuando termines de trabajar, no dejes los contenedores consumiendo RAM. ApÃ¡galos con:

```bash
cd docker
sudo docker compose down
```

-----

*Arquitectura configurada por la Persona A.*

-----

## ğŸ¦ MÃ³dulo: Generador de Datos (Producer)

**Estado:** âœ… Implementado por Persona B.

Este mÃ³dulo sustituye a la API real de Twitter/X. Su funciÃ³n es generar **trÃ¡fico sintÃ©tico** constante para asegurar que siempre haya datos entrando al sistema durante la demostraciÃ³n, evitando bloqueos por lÃ­mites de API o pagos.

### ğŸƒğŸ»â€â™‚ï¸ CÃ³mo ejecutar el simulador

Una vez levantada la infraestructura (Docker), abre una terminal nueva y ejecuta:

# Desde la raÃ­z del proyecto
python src/producer/producer.py

---------------------------------------------------------------------
INSTRUCCIONES DE VERIFICACIÃ“N RÃPIDA
---------------------------------------------------------------------
Si querÃ©is comprobar que mi parte funciona sin arrancar Spark todavÃ­a:

1. Abrid una terminal en la raÃ­z del proyecto y lanzad mi script:
   Command: python src/producer/producer.py

2. Abrid OTRA terminal para ver lo que llega a Kafka (BuzÃ³n):
   Command: docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic tweets_topic --from-beginning

NOTA: El paso 2 es solo para testear. Cuando la Persona C tenga el cÃ³digo de Spark listo, 
usaremos Spark para leer, no este comando de consola.