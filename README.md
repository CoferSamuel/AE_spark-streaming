# ğŸš€ Proyecto Big Data: Simulador de Streaming (Kafka + Spark)

Este proyecto implementa una arquitectura de **Big Data en Tiempo Real**. Simulamos un flujo de datos de una red social (tipo Twitter/X) para procesar tendencias (Trending Topics) al instante.

La arquitectura se basa en un **ClÃºster de un solo nodo (Single-Node Cluster)** virtualizado con Docker.

-----

## ğŸ“‹ Estructura del Proyecto

Antes de tocar nada, entiende quÃ© es cada carpeta y archivo:

  * **`docker/docker-compose.yml`**: ğŸ—ï¸ **Infraestructura.** Define los servicios Zookeeper y Kafka y cÃ³mo se conectan. Docker lee este archivo para levantar toda la infraestructura automÃ¡ticamente.
  * **`src/producer/`**: ğŸ“¤ **Productor.** CÃ³digo fuente para simular el envÃ­o de mensajes (tweets).
  * **`src/consumer/`**: ğŸ“¥ **Consumidor.** CÃ³digo fuente para procesar los mensajes (Spark, etc.).
  * **`src/utils/`**: ğŸ› ï¸ **Utilidades.** Funciones auxiliares y configuraciÃ³n.
  * **`tests/tester.py`**: ğŸ§ª **Test.** Script de prueba para verificar la conexiÃ³n con Kafka.
  * **`.gitignore`**: ğŸ—‘ï¸ **Filtro.** Archivos ignorados por Git.
  * **`README.md`**: ğŸ“– **DocumentaciÃ³n principal.**

-----

## ğŸŒ³ Ãrbol de directorios

```text
AE_spark-streaming/
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ tester.py
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

-----

## ğŸ› ï¸ Requisitos Previos

Necesitas tener instalado en tu mÃ¡quina:

1.  **Docker & Docker Compose**: El motor que ejecutarÃ¡ los servidores.
2.  **Python 3.9+**: Recomendamos usar **Anaconda/Miniconda**.
3.  **Git**: Para descargar este cÃ³digo.

-----

## ğŸš€ InstalaciÃ³n y Puesta en Marcha

Sigue estos pasos en orden exacto.

### 1\. Clonar el repositorio

Descarga el cÃ³digo a tu mÃ¡quina:

```bash
git clone <URL_DEL_REPOSITORIO>
cd spark-streaming-project
```

### 2\. Preparar el entorno Python

Vamos a crear un entorno limpio para no mezclar librerÃ­as (es recomendable usar conda, pero no es necesario).

```bash
# Crear entorno llamado 'arqesp'
conda create --name arqesp python=3.9 -y

# Activar el entorno
conda activate arqesp

# Instalar la librerÃ­a para hablar con Kafka
pip install kafka-python
```

### 3\. Levantar la Infraestructura (Docker)

Este comando descargarÃ¡ las imÃ¡genes y encenderÃ¡ Zookeeper y Kafka en segundo plano.

```bash
# Si estÃ¡s en Linux/Mac y requiere permisos, usa 'sudo' delante
cd docker
sudo docker compose up -d
```

*Espera unos segundos hasta que diga "Started" o "Running".*

### 4\. Crear el Canal de ComunicaciÃ³n (Topic)

**âš ï¸ IMPORTANTE:** Este paso solo es necesario hacerlo **una vez** (la primera vez que arrancas el sistema). Creamos el "buzÃ³n" donde se guardarÃ¡n los tweets.

```bash
sudo docker compose exec kafka kafka-topics --create --topic tweets_topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1
```

*Si sale bien, dirÃ¡: `Created topic tweets_topic`.*

-----

## âœ… Verificar que todo funciona

Para asegurarte de que tu ordenador puede hablar con el Kafka que vive dentro de Docker, hemos creado un script de prueba.

AsegÃºrate de tener el entorno activado (`conda activate arqesp`) y ejecuta:

```bash
python tests/tester.py
```

Si ves mensajes con **[âœ”] Enviado** y **[âœ”] Recibido**, Â¡felicidades\! Tu entorno estÃ¡ listo para empezar a desarrollar.

-----

## â„¹ï¸ Datos TÃ©cnicos (Para configuraciÃ³n)

Si necesitas configurar tus scripts (Producer o Spark), usa estos datos:

  * **Servidor Kafka (Bootstrap Server):** `localhost:9092`
  * **Nombre del Topic:** `tweets_topic`
  * **Zookeeper (Interno):** Puerto 2181

-----

## ğŸ›‘ CÃ³mo detener todo

Cuando termines de trabajar, no dejes los contenedores consumiendo RAM. ApÃ¡galos con:

```bash
cd docker
sudo docker compose down
```

-----

*Arquitectura configurada por la Persona A.*