*** NOTAS DE INTEGRACIÓN DEL MÓDULO PRODUCER (PERSONA B) ***



---------------------------------------------------------------------
PARA: PERSONA C (Ingeniero Spark)
ASUNTO: Estructura de datos para el Consumer
---------------------------------------------------------------------
¡Hola! El Producer ya está enviando datos. Aquí tienes los detalles técnicos
que necesitas para tu script de Spark (`readStream`):

1. TOPIC DE KAFKA: "tweets_topic"
2. FORMATO: JSON (codificado en UTF-8)
3. ESQUEMA DEL JSON (Copia estos nombres de campo exactos):
   {
      "usuario": "...",
      "texto": "...", 
      "hashtag_principal": "..."  <-- ¡USA ESTE CAMPO PARA EL GROUPBY!
      "timestamp": ...
   }

CONSEJO:
Cuando hagas el filtro en Spark, no hace falta que hagas un `split` complejo del texto.
Ya te estoy enviando el hashtag limpio en el campo "hashtag_principal".
Solo tienes que agrupar por esa columna y contar.

---------------------------------------------------------------------
PARA: PERSONA D (Analista/Memoria)
ASUNTO: Texto técnico para la memoria y presentación
---------------------------------------------------------------------
Aquí tienes la justificación técnica de mi parte para incluir en la memoria
y explicar en la diapositiva de "Arquitectura/Ingesta":

"IMPLEMENTACIÓN DEL GENERADOR DE DATOS (PRODUCER)"
Para la fase de ingesta de datos, hemos desarrollado un simulador en Python
que emula el comportamiento de una red social en tiempo real.

¿Por qué no usamos la API real de Twitter/X?
1. Independencia: Evitamos depender de claves de API externas que ahora son de pago
   o tienen límites de uso (rate-limits), lo que podría romper la demo en vivo.
2. Control: Al generar datos sintéticos, garantizamos un flujo constante de
   mensajes con hashtags específicos, asegurando que siempre haya "Trending Topics"
   que mostrar en la visualización de Spark.

Detalles técnicos:
- Lenguaje: Python (librería kafka-python).
- Serialización: JSON.
- Frecuencia: 1 mensaje/segundo (simulando tráfico humano real).
- Flujo: El script actúa como 'Productor' en la arquitectura Kafka, inyectando
  datos en el topic 'tweets_topic' que actúa como buffer intermedio."

---------------------------------------------------------------------
EVIDENCIA DE FUNCIONAMIENTO
(Adjuntar aquí o en la carpeta /img la captura de pantalla de las dos terminales
que guardé previamente).